{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Collab and Get Parquet Files"
      ],
      "metadata": {
        "id": "jpgbTTml5mlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipinfo.io"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQltgcsRJ9jQ",
        "outputId": "947d13c6-e2d5-4fa5-b5e9-2ae062cffd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ip\": \"34.45.78.225\",\n",
            "  \"hostname\": \"225.78.45.34.bc.googleusercontent.com\",\n",
            "  \"city\": \"Council Bluffs\",\n",
            "  \"region\": \"Iowa\",\n",
            "  \"country\": \"US\",\n",
            "  \"loc\": \"41.2619,-95.8608\",\n",
            "  \"org\": \"AS396982 Google LLC\",\n",
            "  \"postal\": \"51502\",\n",
            "  \"timezone\": \"America/Chicago\",\n",
            "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnP3xNtg5sAO",
        "outputId": "9fb4f4a5-01d3-4e77-e385-de72215429d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIA2YybS5xT5",
        "outputId": "efe715e0-d38d-45b9-c56f-f703990173a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RY3jEKG-8v4",
        "outputId": "37d03c93-1b18-4b12-ec59-d828ce987c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2.5M\n",
            "drwx------ 5 root root 4.0K Nov 14 21:15 drive\n",
            "-rw-r--r-- 1 root root 2.5M Nov 14 21:15 eth_news_data.parquet.gzip\n",
            "drwxr-xr-x 1 root root 4.0K Nov 12 14:30 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/*.parquet.gzip \"/content/drive/MyDrive/KFUPM Study Material/Term 3 (251 - 2025)/ICS-590/crypto-forecasting-public/big-datasets\""
      ],
      "metadata": {
        "id": "F8ogfhuQ5tmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google News Data Fetch"
      ],
      "metadata": {
        "id": "I_WyaXsk3K6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "E73FYhCC3Qn1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BKK1rTv3Hk9",
        "outputId": "358a6fdf-9814-4253-d8e5-9f48bdbe8e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google News functions loaded.\n"
          ]
        }
      ],
      "source": [
        "''' Functions for Google News data fetching. '''\n",
        "\n",
        "import sys\n",
        "sys.path.append('../../')\n",
        "\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from functools import lru_cache\n",
        "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
        "from email.utils import parsedate_to_datetime\n",
        "import warnings\n",
        "\n",
        "# Quiet the BS4 XML warning (we are explicitly parsing as XML)\n",
        "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
        "\n",
        "# --- HTTP defaults (helps avoid sporadic empty feeds / blocks)\n",
        "_REQ_TIMEOUT = 20\n",
        "_REQ_HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "\n",
        "class GoogleNewsRSS:\n",
        "    ''' Scraper for Google News RSS. '''\n",
        "\n",
        "    def __init__(self, rss_url: str):\n",
        "        self.url = rss_url\n",
        "        try:\n",
        "            self.response = requests.get(rss_url, headers=_REQ_HEADERS, timeout=_REQ_TIMEOUT)\n",
        "            self.response.raise_for_status()\n",
        "        except Exception as e:\n",
        "            # Hard fail here; caller will handle\n",
        "            raise RuntimeError(f\"Request failed for Google News RSS: {rss_url} :: {e}\")\n",
        "\n",
        "        # Parse as XML (not HTML)\n",
        "        try:\n",
        "            self.soup = BeautifulSoup(self.response.text, features=\"xml\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Could not parse XML from: {rss_url} :: {e}\")\n",
        "\n",
        "        # Always define self.articles\n",
        "        channel = self.soup.find('channel')\n",
        "        self.articles = [] if channel is None else channel.find_all('item')\n",
        "\n",
        "        # Normalize per-item fields (be robust to tag name differences)\n",
        "        def _get_text(tag):\n",
        "            return tag.get_text(strip=True) if tag else None\n",
        "\n",
        "        def _pubdate(item):\n",
        "            # Google uses <pubDate>; be tolerant of case variants\n",
        "            tag = item.find('pubDate') or item.find('pubdate')\n",
        "            return _get_text(tag)\n",
        "\n",
        "        self.articles_dicts = [{\n",
        "            'title': _get_text(item.find('title')),\n",
        "            'link': _get_text(item.find('link')),\n",
        "            'description': _get_text(item.find('description')),\n",
        "            'pubdate': _pubdate(item),\n",
        "        } for item in self.articles]\n",
        "\n",
        "        self.size = len(self.articles)\n",
        "        self.urls = [d['link'] for d in self.articles_dicts if d.get('link')]\n",
        "        self.titles = [d['title'] for d in self.articles_dicts if d.get('title')]\n",
        "        self.descriptions = [d['description'] for d in self.articles_dicts if d.get('description')]\n",
        "        self.publication_times = [d['pubdate'] for d in self.articles_dicts if d.get('pubdate')]\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "def convert_time(time_str: str) -> datetime:\n",
        "    \"\"\"\n",
        "    Google News RSS uses RFC2822 dates, e.g.:\n",
        "    'Tue, 04 Mar 2025 12:34:56 GMT'\n",
        "    Use email.utils.parsedate_to_datetime for reliability.\n",
        "    \"\"\"\n",
        "    if not time_str:\n",
        "        raise ValueError(\"Empty pubdate string\")\n",
        "    try:\n",
        "        return parsedate_to_datetime(time_str)\n",
        "    except Exception:\n",
        "        # Fallback to the original regex-based approach if needed\n",
        "        _format = '%d %b %Y %H:%M:%S'\n",
        "        cleaned = re.sub(r'^.*?,', ',', time_str)[2:][:-4]\n",
        "        return datetime.strptime(cleaned, _format)\n",
        "\n",
        "\n",
        "def get_data(coin: str = 'BTC') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a date-bounded Google News RSS query for BTC/ETH and\n",
        "    aggregate daily results into a single DataFrame indexed by timestamp.\n",
        "    \"\"\"\n",
        "    link_tmpl = string.Template(\n",
        "        'https://news.google.com/rss/search?'\n",
        "        'q=CoinDesk+OR+Cointelegraph+OR+Decrypt,+$currency+OR+$symbol+'\n",
        "        'after:$early_date+before:$late_date&ceid=US:en&hl=en-US&gl=US'\n",
        "    )\n",
        "\n",
        "    if coin == 'BTC':\n",
        "        currency = 'Bitcoin'\n",
        "    elif coin == 'ETH':\n",
        "        currency = 'Ethereum'\n",
        "    else:\n",
        "        raise ValueError(f'Coin not supported: {coin}')\n",
        "\n",
        "    all_data = pd.DataFrame()\n",
        "\n",
        "    # Start date as in your current code for BTC\n",
        "    # c_date = datetime.strptime('01-01-2009', '%d-%m-%Y')\n",
        "    # Start date as in your current code for ETH\n",
        "    c_date = datetime.strptime('01-01-2013', '%d-%m-%Y')\n",
        "\n",
        "    # Shuffle daily windows to reduce rate limits\n",
        "    iterator = []\n",
        "    while c_date <= datetime.now():\n",
        "        iterator.append(c_date)\n",
        "        c_date += timedelta(days=1)\n",
        "    random.shuffle(iterator)\n",
        "\n",
        "    for date in iterator:\n",
        "        next_date = date + timedelta(days=1)\n",
        "        url = link_tmpl.substitute(\n",
        "            currency=currency,\n",
        "            symbol=coin,\n",
        "            early_date=date.strftime('%Y-%m-%d'),\n",
        "            late_date=next_date.strftime('%Y-%m-%d'),\n",
        "        )\n",
        "\n",
        "        # req = GoogleNewsRSS(url)\n",
        "        try:\n",
        "            req = GoogleNewsRSS(url)\n",
        "        except RuntimeError as e:\n",
        "            # If it's a 503 or 429, log and skip that day; otherwise re-raise\n",
        "            msg = str(e)\n",
        "            if '503' in msg or '429' in msg:\n",
        "                print(f\"Transient error for {coin} {date.date()}: {msg} â€“ skipping this day.\")\n",
        "                continue\n",
        "            else:\n",
        "                raise RuntimeError(f\"Request failed for Google News RSS due to {e}\")\n",
        "\n",
        "        # No items for that window â†’ skip\n",
        "        if not req.publication_times:\n",
        "            continue\n",
        "\n",
        "        rows = list(zip(req.publication_times, req.titles, req.urls))\n",
        "        c_df = pd.DataFrame(rows, columns=['time', 'title', 'url'])\n",
        "\n",
        "        # Parse timestamps\n",
        "        c_df['datetime'] = [convert_time(t) for t in c_df.time]\n",
        "        c_df['timestamp'] = [dt.timestamp() for dt in c_df.datetime]\n",
        "        c_df = c_df.drop(columns='time').set_index('timestamp')\n",
        "\n",
        "        all_data = pd.concat([all_data, c_df], axis=0, ignore_index=False)\n",
        "\n",
        "    print(f'All elements fetched. ({len(iterator)}/{len(iterator)})')\n",
        "    if all_data.empty:\n",
        "        # Make the failure explicit to the caller/runner\n",
        "        raise RuntimeError(\"Google News RSS returned no rows for the chosen date range.\")\n",
        "    return all_data.sort_index().drop_duplicates()\n",
        "\n",
        "print(\"Google News functions loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Call"
      ],
      "metadata": {
        "id": "8nFqRe3K3Z4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#     btc_news_data = get_data('BTC')\n",
        "#     btc_news_data.to_parquet('btc_news_data.parquet.gzip', compression='gzip')\n",
        "#     print('Bitcoin news data successfully scraped \\U0001F389 \\U0001F389')\n",
        "# except Exception as e:\n",
        "#     print('Error raised during bitcoin news data scraping: ', e)\n",
        "\n",
        "try:\n",
        "    eth_news_data = get_data('ETH')\n",
        "    eth_news_data.to_parquet('eth_news_data.parquet.gzip', compression='gzip')\n",
        "    print('Ethereum news data successfully scraped \\U0001F389 \\U0001F389')\n",
        "except Exception as e:\n",
        "    print('Error raised during ethereum news data scraping: ', e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWxOWtre3cBD",
        "outputId": "b258a887-712a-482a-d0c3-a032fc616d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All elements fetched. (6162/6162)\n",
            "Bitcoin news data successfully scraped ðŸŽ‰ ðŸŽ‰\n",
            "Error raised during ethereum news data scraping:  Request failed for Google News RSS: https://news.google.com/rss/search?q=CoinDesk+OR+Cointelegraph+OR+Decrypt,+Ethereum+OR+ETH+after:2012-03-14+before:2012-03-15&ceid=US:en&hl=en-US&gl=US :: 503 Server Error: Service Unavailable for url: https://news.google.com/rss/search?q=CoinDesk+OR+Cointelegraph+OR+Decrypt,+Ethereum+OR+ETH+after:2012-03-14+before:2012-03-15&ceid=US:en&hl=en-US&gl=US\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    eth_news_data = get_data('ETH')\n",
        "    eth_news_data.to_parquet('eth_news_data.parquet.gzip', compression='gzip')\n",
        "    print('Ethereum news data successfully scraped \\U0001F389 \\U0001F389')\n",
        "except Exception as e:\n",
        "    print('Error raised during ethereum news data scraping: ', e)"
      ],
      "metadata": {
        "id": "yg9-zNqM_Ths",
        "outputId": "592bfda5-438e-4eb3-8328-1ceac47f466b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All elements fetched. (4701/4701)\n",
            "Ethereum news data successfully scraped ðŸŽ‰ ðŸŽ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Trends Data"
      ],
      "metadata": {
        "id": "Zr5nOx1I4MMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "WiXD569g4SkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explore_url = \"https://trends.google.com/trends/api/explore\"\n",
        "widget_url = 'https://trends.google.com/trends/api/widgetdata/multiline/csv'\n",
        "\n",
        "cookies = {  # TODODEV: Replace with fresh cookies if requests fail\n",
        "    'AEC': 'ARSKqsJPmyKVB...', # Placeholder for a real cookie value\n",
        "}\n",
        "headers = {  # TODODEV: Replace with fresh headers if requests fail\n",
        "    'Host': 'trends.google.com',\n",
        "    'Cookie': 'SID=g.a0002wiTNNCLxuBBFp3bme_jNqAuarl-iXHSwqbOHgUi5nDOVzibEDsiE1j66S2J90FQzYk6JgACgYKAYMSARYSFQHGX2MiRvI4RyQjCEumvcNJnDdTQRoVAUF8yKpQ55Gi3zTLn2wlWBOtf3Rv0076; __Secure-1PSID=g.a0002wiTNNCLxuBBFp3bme_jNqAuarl-iXHSwqbOHgUi5nDOVzibRMncR1cDVMHs9Ztdp8HqDgACgYKAbsSARYSFQHGX2MiQAuYYNmyAgCA8ZIgpAZBkBoVAUF8yKriDy_SPAkxplM9F0RriJJ20076; __Secure-3PSID=g.a0002wiTNNCLxuBBFp3bme_jNqAuarl-iXHSwqbOHgUi5nDOVzibVEWjkPP3f6K2TTgLL7yAxAACgYKAbYSARYSFQHGX2Mi45UbnvYsyp2ZYeuB5KaHJxoVAUF8yKqhubT8FrXSC51W2HCtFRtq0076; HSID=Az5DDIxBoZRWcNygu; SSID=AWGK1Y7qTITh0fQ-W; APISID=R6iPJ3t61HiJE2y3/AvIfC_e82kTUWE2wE; SAPISID=fBGtP9UIP8jh8nO5/A7lfg7aVR3K3BshN3; __Secure-1PAPISID=fBGtP9UIP8jh8nO5/A7lfg7aVR3K3BshN3; __Secure-3PAPISID=fBGtP9UIP8jh8nO5/A7lfg7aVR3K3BshN3; SEARCH_SAMESITE=CgQIoZ8B; AEC=AaJma5v35HSEDNr_n11dzInL9nRcMbgD4QOpcwFNMulHvR77GgQUaLfM8A; __Secure-STRP=ADq1D7ooCPPAX7__1oiug9RgTrbde_TRwdXpEE3GYo27v6xs9jiaOLA5isq2D_6-Bo9t1_KZIv4WvObeEqQua_9Q3pHcCd3JJVkP; __Secure-1PSIDTS=sidts-CjEBwQ9iI82vc7lZm3Mgcmuxolge6fRezWJx1jM7UHD6BCe9DihgtdLxQw26wOI6Nh1gEAA; __Secure-3PSIDTS=sidts-CjEBwQ9iI82vc7lZm3Mgcmuxolge6fRezWJx1jM7UHD6BCe9DihgtdLxQw26wOI6Nh1gEAA; NID=526=flH9BCywvl1cP6Rujl1g8zD0cgRcmgt8IirrE1Ae_fA1fYd9vPD1IgYODkDc5sMD8RN_oqBs8LWKg1zb0eq9Ux8ZGP1iWQ-aJRtv3-wu0HK6ATJrotEuvbWSETlfLo3eFiTFFo5R5VNDwjpLIkaTn4B9t1lz64bUfWONyJWMZ0v_s4Xqt-gjXtcu0hJd1giiTtgiod5EOZ7oF0siw6_AqV0Yij9wyHJspCHOPVDwKKREyQyaQZyuW5NbKfJD-teldbFjQbBHg03wFlbZLsFNEcdrtL0ik9zsT3UVuFXp8dFIPJS7Pxpz_5svTaXPDwn5XE2aUO_kwcBraOvsE5NSJ26lrFjVKo7tVZHYUDLa1YJFvPmE2xm7b-0QN7s; SIDCC=AKEyXzWvCKIhLI0dLYGcdPiLt0YMjOVjKiZwZNyMESYYF_N9xv_dQHs1rut-c5jI1LAyl-S-7w; __Secure-1PSIDCC=AKEyXzVt18wgIHH2vTQvk8FUC_H5_FzSSu484o4attajftgAn09Zh9Qzdi_0T3iJeZv-AUFemuI; __Secure-3PSIDCC=AKEyXzXi6WgXABCYvMN8nFCHMmG39i3gcoe8j_3UY97W_AHM_usPR6U6VzUEwcEc4mM7SJtjEmk',\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',\n",
        "}\n",
        "print(\"Google Trends config loaded (update TODODEV cookies/headers if needed).\")\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('../../')\n",
        "\n",
        "import time, json, urllib, requests, pandas as pd\n",
        "from io import StringIO\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_explore_api(hl, timezone, keyword, from_date, to_date):\n",
        "    # Ensure keyword is URL-encoded for safety\n",
        "    encoded_keyword = urllib.parse.quote(str(keyword))\n",
        "    # 'time' parameter in req should use 'YYYY-MM-DD YYYY-MM-DD' format\n",
        "    time_param = f\"{datetime.strftime(from_date,'%Y-%m-%d')} {datetime.strftime(to_date,'%Y-%m-%d')}\"\n",
        "    req_payload = {\n",
        "        \"comparisonItem\":[{\"keyword\": encoded_keyword,\"geo\":\"\",\"time\": time_param}],\n",
        "        \"category\":0,\n",
        "        \"property\":\"\"\n",
        "    }\n",
        "    query = urllib.parse.urlencode([\n",
        "        (\"hl\", hl),\n",
        "        (\"tz\", timezone),\n",
        "        (\"req\", json.dumps(req_payload))\n",
        "    ])\n",
        "    response = requests.get(explore_url + '?' + query, cookies=cookies, headers=headers)\n",
        "    while response.status_code == 429:\n",
        "        time.sleep(2) # Wait and retry for rate limits\n",
        "        response = requests.get(explore_url + '?' + query, cookies=cookies, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Explore API returned error {response.status_code}: {response.text}')\n",
        "    # The JSON response starts with ')]}' + a newline, so skip that.\n",
        "    return json.loads(response.text[5:])['widgets']\n",
        "\n",
        "def fetch_widget_api(params: dict):\n",
        "    # widget_url is already defined globally\n",
        "    response = requests.get(widget_url, params=params, cookies=cookies, headers=headers)\n",
        "    while response.status_code == 429:\n",
        "        time.sleep(2) # Wait and retry for rate limits\n",
        "        response = requests.get(widget_url, params=params, cookies=cookies, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f'Widget API returned error {response.status_code}: {response.text}')\n",
        "    return response # Return the response object, content will be accessed in get_trends_data\n",
        "\n",
        "def get_trends_data(keywords: list, begin_date: datetime=None, timezone: str=str(-60), hl: str='en'):\n",
        "    if begin_date is None:\n",
        "        begin_date = datetime.strptime('2009-01-01', '%Y-%m-%d').date()\n",
        "    end_date = datetime.now().date()\n",
        "    all_data = pd.DataFrame()\n",
        "\n",
        "    for keyword in keywords:\n",
        "        current_to_date = end_date\n",
        "        current_from_date = end_date - timedelta(days=250)\n",
        "\n",
        "        keyword_dfs = [] # Collect dataframes for the current keyword\n",
        "\n",
        "        # Loop to fetch data in 250-day windows backwards in time\n",
        "        while True:\n",
        "            # Ensure from_date doesn't go before begin_date\n",
        "            if current_from_date < begin_date:\n",
        "                current_from_date = begin_date\n",
        "\n",
        "            # If current_to_date is before current_from_date, we're done or there's an issue\n",
        "            if current_to_date < current_from_date:\n",
        "                break\n",
        "\n",
        "            # Handle cases where current_from_date and current_to_date are the same or to_date is earlier than from_date\n",
        "            if current_from_date > current_to_date:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                widgets = fetch_explore_api(hl, timezone, keyword, current_from_date, current_to_date)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching explore API for keyword '{keyword}' ({current_from_date} to {current_to_date}): {e}\")\n",
        "                # Adjust dates to try the next window\n",
        "                if current_from_date == begin_date: # Cannot go further back\n",
        "                    break\n",
        "                current_to_date = current_from_date - timedelta(days=1)\n",
        "                current_from_date = current_to_date - timedelta(days=250)\n",
        "                continue\n",
        "\n",
        "\n",
        "            if not widgets: # Handle case where widgets might be empty\n",
        "                print(f\"No widgets returned for keyword '{keyword}' for period {current_from_date} to {current_to_date}. Skipping.\")\n",
        "                if current_from_date == begin_date:\n",
        "                    break\n",
        "                current_to_date = current_from_date - timedelta(days=1)\n",
        "                current_from_date = current_to_date - timedelta(days=250)\n",
        "                continue\n",
        "\n",
        "            token = widgets[0]['token']\n",
        "            request = widgets[0]['request']\n",
        "            params = {'req': json.dumps(request),'token': token,'tz': timezone}\n",
        "\n",
        "            try:\n",
        "                response = fetch_widget_api(params)\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching widget API for keyword '{keyword}' ({current_from_date} to {current_to_date}): {e}\")\n",
        "                if current_from_date == begin_date:\n",
        "                    break\n",
        "                current_to_date = current_from_date - timedelta(days=1)\n",
        "                current_from_date = current_to_date - timedelta(days=250)\n",
        "                continue\n",
        "\n",
        "            csv_content = response.text\n",
        "\n",
        "            # Find the actual CSV header (e.g., \"Day\",\"bitcoin\")\n",
        "            csv_lines = csv_content.strip().split('\\n')\n",
        "\n",
        "            header_row_index = -1\n",
        "            for i, line in enumerate(csv_lines):\n",
        "                # Look for lines that likely contain header for date and keyword data\n",
        "                if ('\"Day\"' in line or 'Day' in line) and ('\"' + keyword.lower() + '\"' in line.lower() or keyword.lower() in line.lower() or 'Interest' in line):\n",
        "                    header_row_index = i\n",
        "                    break\n",
        "                # A more generic check if keyword not explicitly in header line (e.g., \"Day\",\"Interest\")\n",
        "                elif ('\"Day\"' in line or 'Day' in line) and len(line.split(',')) > 1: # check for at least two columns\n",
        "                    header_row_index = i\n",
        "                    break\n",
        "\n",
        "            if header_row_index == -1:\n",
        "                print(f\"Warning: Could not find 'Day' header for keyword '{keyword}' for period {current_from_date} to {current_to_date}. Skipping this window.\")\n",
        "                # This could happen for very old dates with no data.\n",
        "                if current_from_date == begin_date:\n",
        "                    break\n",
        "                current_to_date = current_from_date - timedelta(days=1)\n",
        "                current_from_date = current_to_date - timedelta(days=250)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Read CSV, setting the first column ('Day' or date-like) as index and parsing as dates\n",
        "                df = pd.read_csv(StringIO('\\n'.join(csv_lines[header_row_index:])), index_col=0, parse_dates=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing CSV for keyword '{keyword}' ({current_from_date} to {current_to_date}): {e}. CSV content sample: {csv_content[:500]} Skipping this window.\")\n",
        "                if current_from_date == begin_date:\n",
        "                    break\n",
        "                current_to_date = current_from_date - timedelta(days=1)\n",
        "                current_from_date = current_to_date - timedelta(days=250)\n",
        "                continue\n",
        "\n",
        "            # Google Trends sometimes returns a 'isPartial' column.\n",
        "            # We are interested in the main trend data.\n",
        "            if 'isPartial' in df.columns:\n",
        "                df = df.drop(columns=['isPartial'])\n",
        "\n",
        "            if len(df.columns) > 1:\n",
        "                # If there are still multiple columns (e.g., if the keyword itself is multi-word or ambiguous in GTrends),\n",
        "                # try to pick the column that matches the keyword or is named 'Interest'.\n",
        "                # Otherwise, default to the first column.\n",
        "                if keyword in df.columns:\n",
        "                    df = df[[keyword]]\n",
        "                elif 'Interest' in df.columns: # Common for single keywords\n",
        "                    df = df[['Interest']]\n",
        "                else:\n",
        "                    df = df.iloc[:, 0:1] # Fallback: take the first data column\n",
        "            elif len(df.columns) == 0:\n",
        "                print(f\"Warning: DataFrame for keyword '{keyword}' for period {current_from_date} to {current_to_date} is empty after processing. Skipping.\")\n",
        "                if current_from_date == begin_date:\n",
        "                    break\n",
        "                current_to_date = current_from_date - timedelta(days=1)\n",
        "                current_from_date = current_to_date - timedelta(days=250)\n",
        "                continue\n",
        "\n",
        "            df.columns = [keyword] # Rename the single data column to the actual keyword\n",
        "            keyword_dfs.append(df)\n",
        "\n",
        "            # Prepare for the next window\n",
        "            current_to_date = current_from_date - timedelta(days=1)\n",
        "            next_from_date_candidate = current_to_date - timedelta(days=250)\n",
        "\n",
        "            if next_from_date_candidate < begin_date:\n",
        "                current_from_date = begin_date\n",
        "            else:\n",
        "                current_from_date = next_from_date_candidate\n",
        "\n",
        "            if current_to_date < begin_date: # If the 'to' date for the next window is before the overall begin_date, we're done\n",
        "                break\n",
        "            # If current_from_date has reached begin_date and we've processed up to current_to_date (which is now begin_date - 1),\n",
        "            # we should break to prevent an infinite loop.\n",
        "            if current_from_date == begin_date and current_to_date < begin_date:\n",
        "                break\n",
        "\n",
        "\n",
        "        # After collecting all windows for a keyword, concatenate them\n",
        "        if keyword_dfs:\n",
        "            complete_kw_data = pd.concat(keyword_dfs)\n",
        "            # Drop duplicates by index (date), keeping the last value (most recent fetch for overlapping periods)\n",
        "            complete_kw_data = complete_kw_data[~complete_kw_data.index.duplicated(keep='last')]\n",
        "            complete_kw_data = complete_kw_data.sort_index()\n",
        "\n",
        "            if all_data.empty:\n",
        "                all_data = complete_kw_data\n",
        "            else:\n",
        "                # Use merge for column-wise concatenation with automatic index alignment\n",
        "                all_data = pd.merge(all_data, complete_kw_data, left_index=True, right_index=True, how='outer')\n",
        "        else:\n",
        "            print(f\"No data fetched for keyword: {keyword}\")\n",
        "\n",
        "    return all_data.sort_index()\n",
        "\n",
        "print(\"Google Trends functions loaded.\")"
      ],
      "metadata": {
        "id": "VAJd294S4SJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac53bfc-c570-4125-b12d-b04d84aeed54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Trends config loaded (update TODODEV cookies/headers if needed).\n",
            "Google Trends functions loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Call"
      ],
      "metadata": {
        "id": "JblANmbD422T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trends_data = get_trends_data(['bitcoin','ethereum','cryptocurrency','blockchain','investing'], timezone=str(0))\n",
        "trends_data.to_parquet('google_trends.parquet.gzip', compression='gzip')"
      ],
      "metadata": {
        "id": "abyq5nKD49cX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5300e042-1fcd-44fc-d99e-8cc8966d4a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4282577138.py:197: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
            "  complete_kw_data = pd.concat(keyword_dfs)\n",
            "/tmp/ipython-input-4282577138.py:197: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
            "  complete_kw_data = pd.concat(keyword_dfs)\n",
            "/tmp/ipython-input-4282577138.py:197: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
            "  complete_kw_data = pd.concat(keyword_dfs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yahoo Finance Data Fetch"
      ],
      "metadata": {
        "id": "dr2RytcC5BGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions and Call"
      ],
      "metadata": {
        "id": "CtDhtaIg5Dpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "start_date='2009-01-01'\n",
        "end_date='2025-10-31'\n",
        "\n",
        "sp500_data = yf.download(\"^GSPC\", start=start_date, end=end_date)\n",
        "vix_data   = yf.download(\"^VIX\",  start=start_date, end=end_date)\n",
        "gold_data  = yf.download(\"GC=F\",  start=start_date, end=end_date)\n",
        "\n",
        "# Safely pick Adj Close if available, otherwise Close\n",
        "sp500_close = sp500_data.get('Adj Close', sp500_data['Close'])\n",
        "vix_close   = vix_data.get('Adj Close', vix_data['Close'])\n",
        "gold_close  = gold_data.get('Adj Close', gold_data['Close'])\n",
        "\n",
        "yf_data = pd.concat([sp500_close, sp500_data['Volume'],\n",
        "                     vix_close, gold_close], axis=1)\n",
        "\n",
        "yf_data.columns = ['sp500_price', 'sp500_volume', 'vix_price', 'gold_usd_price']\n",
        "\n",
        "yf_data.to_parquet('yf_data.parquet.gzip', compression='gzip')\n",
        "\n",
        "print(\"âœ… Yahoo Finance data saved to yf_data.parquet.gzip\")\n"
      ],
      "metadata": {
        "id": "L77aUmY55Fdt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c5d896-dbd6-4ff7-d739-d072356dd7ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-271872396.py:17: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  sp500_data = yf.download(\"^GSPC\", start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-271872396.py:18: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  vix_data   = yf.download(\"^VIX\",  start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-271872396.py:19: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  gold_data  = yf.download(\"GC=F\",  start=start_date, end=end_date)\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Yahoo Finance data saved to yf_data.parquet.gzip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting BTC and ETH CSV to Parquet"
      ],
      "metadata": {
        "id": "gwpLtcm2w9ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/KFUPM Study Material/Term 3 (251 - 2025)/ICS-590/crypto-forecasting-public/big-datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD3ceL8_xV3P",
        "outputId": "13bc4d51-c77a-4cdf-8535-af29da26fd04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/KFUPM Study Material/Term 3 (251 - 2025)/ICS-590/crypto-forecasting-public/big-datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRxeJfQxrGJ",
        "outputId": "f51105e7-19ce-474b-a7b6-65ec69490181"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12M\n",
            "-rw------- 1 root root 463K Nov 14 21:43  bitcoin_2010-07-17_2024-06-28.csv\n",
            "-rw------- 1 root root 293K Nov 14 23:50  btc_data.parquet\n",
            "-rw------- 1 root root 3.5M Nov 14 20:01  btc_news_data.parquet.gzip\n",
            "-rw------- 1 root root  38K Nov 24 11:21 'Data Acquisition'\n",
            "-rw------- 1 root root 219K Nov 14 23:50  eth_data.parquet\n",
            "-rw------- 1 root root 339K Nov 14 21:43  ethereum_2015-08-07_2024-06-28.csv\n",
            "-rw------- 1 root root 2.5M Nov 14 21:15  eth_news_data.parquet.gzip\n",
            "-rw------- 1 root root 3.6M Nov 19 14:46  Fungineering.ipynb\n",
            "-rw------- 1 root root  68K Nov 14 20:22  google_trends.parquet.gzip\n",
            "-rw------- 1 root root  37K Nov 18 22:48  histories_classification_before_stopping.pkl\n",
            "-rw------- 1 root root  16K Nov 19 13:25  histories_classification.pkl\n",
            "-rw------- 1 root root 8.5K Nov 19 14:46  histories_regression_modified_new.pkl\n",
            "-rw------- 1 root root 8.5K Nov 18 23:21  histories_regression_modified.pkl\n",
            "-rw------- 1 root root 4.4K Nov 19 13:29  histories_regression_new.pkl\n",
            "-rw------- 1 root root 4.5K Nov 18 23:16  histories_regression.pkl\n",
            "-rw------- 1 root root  886 Nov 19 14:01  histories_transformer_classification_new.pkl\n",
            "-rw------- 1 root root  886 Nov 18 23:10  histories_transformer_classification.pkl\n",
            "drwx------ 2 root root 4.0K Nov 15 01:29  outputs_big_experiment\n",
            "-rw------- 1 root root 103K Nov 14 20:23  yf_data.parquet.gzip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define input and output file paths\n",
        "csv_file_path = 'bitcoin_2010-07-17_2024-06-28.csv'\n",
        "parquet_file_path = 'btc_data.parquet'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Write the DataFrame to a Parquet file\n",
        "df.to_parquet(parquet_file_path, index=False) # index=False prevents writing the DataFrame index as a column\n",
        "print(f\"CSV file '{csv_file_path}' converted to Parquet file '{parquet_file_path}' successfully.\")\n",
        "\n",
        "# Define input and output file paths\n",
        "csv_file_path = 'ethereum_2015-08-07_2024-06-28.csv'\n",
        "parquet_file_path = 'eth_data.parquet'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Write the DataFrame to a Parquet file\n",
        "df.to_parquet(parquet_file_path, index=False) # index=False prevents writing the DataFrame index as a column\n",
        "print(f\"CSV file '{csv_file_path}' converted to Parquet file '{parquet_file_path}' successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm-7Gpn9xBpD",
        "outputId": "b6417ed9-9c24-416e-fde5-5c18d123e289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'bitcoin_2010-07-17_2024-06-28.csv' converted to Parquet file 'btc_data.parquet' successfully.\n",
            "CSV file 'ethereum_2015-08-07_2024-06-28.csv' converted to Parquet file 'eth_data.parquet' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your Parquet file\n",
        "btc_parquet_file_path = 'btc_data.parquet'\n",
        "eth_parquet_file_path = 'eth_data.parquet'\n",
        "btc_news_data_parquet_file_path = 'btc_news_data.parquet.gzip'\n",
        "eth_news_data_parquet_file_path = 'eth_news_data.parquet.gzip'\n",
        "google_trends_parquet_file_path = 'google_trends.parquet.gzip'\n",
        "yf_data_parquet_file_path = 'yf_data.parquet.gzip'\n",
        "\n",
        "df = pd.read_parquet(btc_parquet_file_path)\n",
        "print(\"BTC DATA\")\n",
        "print(df.columns)\n",
        "print(df.head(1))\n",
        "\n",
        "df = pd.read_parquet(eth_parquet_file_path)\n",
        "print(\"\\nETH DATA\")\n",
        "print(df.columns)\n",
        "print(df.head(1))\n",
        "\n",
        "df = pd.read_parquet(btc_news_data_parquet_file_path)\n",
        "print(\"\\nBTC NEWS DATA\")\n",
        "print(df.columns)\n",
        "print(df.head(1))\n",
        "\n",
        "df = pd.read_parquet(eth_news_data_parquet_file_path)\n",
        "print(\"\\nETH NEWS DATA\")\n",
        "print(df.columns)\n",
        "print(df.head(1))\n",
        "\n",
        "df = pd.read_parquet(google_trends_parquet_file_path)\n",
        "print(\"\\nGOOGLE TRENDS DATA\")\n",
        "print(df.columns)\n",
        "print(df.head(1))\n",
        "\n",
        "df = pd.read_parquet(yf_data_parquet_file_path)\n",
        "print(\"\\nYAHOO FINANCE DATA\")\n",
        "print(df.columns)\n",
        "print(df.head(1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNqejjq4yPGK",
        "outputId": "714fdfad-5d3c-417e-eece-cd78eb14bc3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BTC DATA\n",
            "Index(['Start', 'End', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market Cap'], dtype='object')\n",
            "        Start         End     Open      High       Low     Close  \\\n",
            "0  2024-06-27  2024-06-28  60882.9  62338.43  60666.19  61646.05   \n",
            "\n",
            "         Volume    Market Cap  \n",
            "0  1.215887e+11  1.208392e+12  \n",
            "\n",
            "ETH DATA\n",
            "Index(['Start', 'End', 'Open', 'High', 'Low', 'Close', 'Volume', 'Market Cap'], dtype='object')\n",
            "        Start         End       Open       High        Low      Close  \\\n",
            "0  2024-06-27  2024-06-28  3383.5292  3481.1707  3379.1649  3458.8487   \n",
            "\n",
            "         Volume    Market Cap  \n",
            "0  1.580400e+10  4.116605e+11  \n",
            "\n",
            "BTC NEWS DATA\n",
            "Index(['title', 'url', 'datetime'], dtype='object')\n",
            "                                          title  \\\n",
            "timestamp                                         \n",
            "1.365404e+09  How To Mine Bitcoins - TechCrunch   \n",
            "\n",
            "                                                            url  \\\n",
            "timestamp                                                         \n",
            "1.365404e+09  https://news.google.com/rss/articles/CBMiZkFVX...   \n",
            "\n",
            "                              datetime  \n",
            "timestamp                               \n",
            "1.365404e+09 2013-04-08 07:00:00+00:00  \n",
            "\n",
            "ETH NEWS DATA\n",
            "Index(['title', 'url', 'datetime'], dtype='object')\n",
            "                                                          title  \\\n",
            "timestamp                                                         \n",
            "1.366009e+09  Meet the money behind bitcoin competitor OpenC...   \n",
            "\n",
            "                                                            url  \\\n",
            "timestamp                                                         \n",
            "1.366009e+09  https://news.google.com/rss/articles/CBMimAFBV...   \n",
            "\n",
            "                              datetime  \n",
            "timestamp                               \n",
            "1.366009e+09 2013-04-15 07:00:00+00:00  \n",
            "\n",
            "GOOGLE TRENDS DATA\n",
            "Index(['bitcoin', 'ethereum', 'cryptocurrency', 'blockchain', 'investing'], dtype='object')\n",
            "            bitcoin  ethereum  cryptocurrency  blockchain  investing\n",
            "Day                                                                 \n",
            "2009-01-01        0       0.0             NaN         NaN         78\n",
            "\n",
            "YAHOO FINANCE DATA\n",
            "Index(['sp500_price', 'sp500_volume', 'vix_price', 'gold_usd_price'], dtype='object')\n",
            "            sp500_price  sp500_volume  vix_price  gold_usd_price\n",
            "Date                                                            \n",
            "2009-01-02   931.799988  4.048270e+09  39.189999      878.799988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the directory containing your Parquet files\n",
        "parquet_dir = './'\n",
        "# Define the directory where you want to save the CSV files\n",
        "csv_dir = 'csv_files'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(csv_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through all files in the Parquet directory\n",
        "for filename in os.listdir(parquet_dir):\n",
        "    if filename.endswith('.parquet.gzip'):\n",
        "        parquet_filepath = os.path.join(parquet_dir, filename)\n",
        "\n",
        "        # Read the Parquet file into a Pandas DataFrame\n",
        "        df = pd.read_parquet(parquet_filepath)\n",
        "\n",
        "        # Construct the output CSV filename\n",
        "        csv_filename = filename.replace('.parquet.gzip', '.csv')\n",
        "        csv_filepath = os.path.join(csv_dir, csv_filename)\n",
        "\n",
        "        # Write the DataFrame to a CSV file (without index)\n",
        "        df.to_csv(csv_filepath, index=False)\n",
        "        print(f\"Converted '{filename}' to '{csv_filename}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "037JVuq4mkOm",
        "outputId": "3c7c2bef-aee9-4830-957e-f552d8080fe9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 'btc_news_data.parquet.gzip' to 'btc_news_data.csv'\n",
            "Converted 'google_trends.parquet.gzip' to 'google_trends.csv'\n",
            "Converted 'yf_data.parquet.gzip' to 'yf_data.csv'\n",
            "Converted 'eth_news_data.parquet.gzip' to 'eth_news_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_TgjPmrWmDLk"
      }
    }
  ]
}